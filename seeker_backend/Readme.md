# The Seeker - Backend API

This directory contains the backend services for **The Seeker** application. The backend is responsible for two primary functions:

1.  **Pushing Data:** Ingesting new job data from the scraping module and storing it in a MySQL database.
2.  **Pulling Data:** Serving the job data via a REST API to the frontend.

The backend is built using **FastAPI**, a modern Python web framework, and utilizes **MySQL** for data storage.

-----

## üìÇ Directory Structure

```
seeker_backend/
‚îú‚îÄ‚îÄ pull_api.py             # FastAPI application for serving job data
‚îú‚îÄ‚îÄ push_jobs.py            # Script for populating the database with new jobs
‚îú‚îÄ‚îÄ Readme.md               # This documentation file
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ venv/                   # Python virtual environment
‚îú‚îÄ‚îÄ .env                    # Environment variables for database credentials
‚îî‚îÄ‚îÄ push_cron.log           # Log file for the cron job
```

-----

## ‚öôÔ∏è How It Works

### **`push_jobs.py`**

This script is the data ingestion component of the backend. It's designed to be run periodically as a **cron job**, to keep the database up-to-date with the latest job postings. The script performs the following actions:

1.  **Loads Environment Variables:** It reads database credentials (host, user, password, database name) from a `.env` file for secure configuration.
2.  **Reads Scraped Data:** It opens and reads the `all_jobs.json` file generated by the scraper module, which contains all the latest job postings.
3.  **Connects to MySQL:** It establishes a connection to the MySQL database using the loaded credentials.
4.  **Clears Existing Data:** It deletes all records from the `jobs` table to ensure that the database only contains the most recent data.
5.  **Performs Bulk Insert:** It prepares the new job data and performs a bulk insert into the `jobs` table, which is an efficient way to add multiple rows at once.
6.  **Closes Connection:** It closes the database connection.

This process ensures the database is consistently refreshed with the latest job information without accumulating outdated entries.

### **`pull_api.py`**

This script is the **FastAPI** application that serves as the public-facing API for the frontend. It is designed to handle requests for job data.

  * **CORS Middleware:** It includes **Cross-Origin Resource Sharing (CORS)** middleware, allowing the API to be accessed securely from different domains, such as the local development server and the GitHub Pages-hosted frontend. The `allow_origins=["*"]` setting is used for simplicity and flexibility.
  * **Database Connection:** It defines a `get_db_connection()` function to establish and manage a connection to the MySQL database for each API call.
  * **API Endpoints:** It provides two main endpoints for data retrieval:
      * `GET /api/jobs`: Retrieves a list of all jobs, ordered from the newest to the oldest.
      * `GET /api/jobs/{job_id}`: Retrieves the details of a specific job by its unique ID.

### **Server Deployment and Management**

The `pull_api.py` script is run using **Uvicorn**, an ASGI server, to serve the FastAPI application. To ensure the API remains active and handles traffic efficiently, a process manager is used.

**PM2** (Process Manager 2) is a daemon process manager that helps keep the application running continuously. The following command is used to start the API:

```bash
pm2 start /home/seeker/seeker/seeker_backend/venv/bin/python --name "seeker_api" --cwd "/home/seeker/seeker/seeker_backend/" -- /home/seeker/seeker/seeker_backend/venv/bin/uvicorn pull_api:app --host 0.0.0.0 --port 8000
```

This command:

  * Starts the Python script from within its virtual environment.
  * Names the process `seeker_api` for easy management.
  * Sets the working directory to `seeker_backend`.
  * Passes arguments to Uvicorn to run the `pull_api:app` with the host `0.0.0.0` (accessible from any network interface) on port `8000`.

For production, the API on port `8000` is then **reverse-proxied** using **Apache**, which allows it to be served securely over a standard domain with an **SSL certificate**. This setup ensures the API is reliable, secure, and always available.